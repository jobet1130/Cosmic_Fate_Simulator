{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d7a63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import astropy.units as u\n",
    "from astropy.cosmology import Planck18\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    PowerTransformer, QuantileTransformer\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97d561e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROJECT_ROOT = Path.cwd().parents[1]  # Goes up to project root from notebooks/data_preprocessing\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "TRANSFORMED_DATA_DIR = PROJECT_ROOT / 'data' / 'transformed'\n",
    "TRANSFORMED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63519ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitConverter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for astronomical unit conversions\"\"\"\n",
    "    def __init__(self, conversions=None):\n",
    "        self.conversions = conversions or {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col, (from_unit, to_unit) in self.conversions.items():\n",
    "            if col in X.columns:\n",
    "                try:\n",
    "                    quantity = X[col].values * u.Unit(from_unit)\n",
    "                    X[col] = quantity.to(u.Unit(to_unit)).value\n",
    "                    print(f\"Converted {col} from {from_unit} to {to_unit}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {col}: {e}\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "04a3a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(numeric_features, categorical_features):\n",
    "    \"\"\"Create preprocessing pipeline for numeric and categorical features\"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features) if numeric_features else ('passthrough', 'passthrough', []),\n",
    "            ('cat', categorical_transformer, categorical_features) if categorical_features else ('passthrough', 'passthrough', [])\n",
    "        ])\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daf65472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df, preprocessor, fit=False):\n",
    "    \"\"\"Apply transformations to the data\"\"\"\n",
    "    if fit:\n",
    "        transformed = preprocessor.fit_transform(df)\n",
    "        # Save the preprocessor\n",
    "        joblib.dump(preprocessor, TRANSFORMED_DATA_DIR / 'preprocessor.joblib')\n",
    "    else:\n",
    "        transformed = preprocessor.transform(df)\n",
    "    \n",
    "    # Get feature names\n",
    "    if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = df.columns.tolist()\n",
    "    \n",
    "    return pd.DataFrame(transformed, columns=feature_names, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88573c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main transformation pipeline\"\"\"\n",
    "    print(\"Starting data transformation...\")\n",
    "    \n",
    "    # Define unit conversions (customize based on your data)\n",
    "    unit_conversions = {\n",
    "        # 'column_name': ('from_unit', 'to_unit')\n",
    "        'distance': ('pc', 'kpc'),\n",
    "        'radius': ('lyr', 'pc'),\n",
    "        'mass': ('solMass', 'kg'),\n",
    "        'luminosity': ('solLum', 'W')\n",
    "    }\n",
    "    \n",
    "    # Load processed data\n",
    "    processed_files = list(PROCESSED_DATA_DIR.glob('*_cleaned.csv'))\n",
    "    if not processed_files:\n",
    "        print(\"No processed data found. Run the cleaning notebook first.\")\n",
    "        return\n",
    "    \n",
    "    for filepath in tqdm(processed_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Load data\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"\\nProcessing {filepath.name}...\")\n",
    "            print(f\"Initial shape: {df.shape}\")\n",
    "            \n",
    "            # Apply unit conversions\n",
    "            converter = UnitConverter(unit_conversions)\n",
    "            df = converter.fit_transform(df)\n",
    "            \n",
    "            # Identify feature types\n",
    "            numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            \n",
    "            # Create and apply preprocessor\n",
    "            preprocessor = create_preprocessor(numeric_features, categorical_features)\n",
    "            df_transformed = transform_data(df, preprocessor, fit=True)\n",
    "            \n",
    "            # Save transformed data\n",
    "            output_path = TRANSFORMED_DATA_DIR / f\"{filepath.stem}_transformed.parquet\"\n",
    "            df_transformed.to_parquet(output_path)\n",
    "            print(f\"Saved transformed data to {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
