{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61b7f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import astropy.units as u\n",
    "from astropy.cosmology import Planck18\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc4fec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "RAW_DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "for directory in [RAW_DATA_DIR, PROCESSED_DATA_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    print(\"Loading raw data files...\")\n",
    "    data = {}\n",
    "    \n",
    "    file_formats = ['*.csv', '*.fits', '*.fits.gz']\n",
    "    for fmt in file_formats:\n",
    "        for filepath in RAW_DATA_DIR.glob(fmt):\n",
    "            try:\n",
    "                if filepath.suffix == '.csv':\n",
    "                    df = pd.read_csv(filepath)\n",
    "                else:\n",
    "                    from astropy.table import Table\n",
    "                    table = Table.read(filepath)\n",
    "                    # Handle multi-dimensional columns for SDSS data\n",
    "                    if 'sdss' in filepath.name.lower():\n",
    "                        names = [name for name in table.colnames if len(table[name].shape) <= 1]\n",
    "                        df = table[names].to_pandas()\n",
    "                    else:\n",
    "                        df = table.to_pandas()\n",
    "                \n",
    "                key = filepath.stem\n",
    "                if key in data:\n",
    "                    key = f\"{key}_{len([k for k in data.keys() if k.startswith(key)])}\"\n",
    "                data[key] = df\n",
    "                print(f\"✓ Loaded {filepath.name} with shape {df.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filepath.name}: {e}\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7634ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    \"\"\"Standardize column names to lowercase with underscores.\"\"\"\n",
    "    df.columns = (df.columns\n",
    "                 .str.lower()\n",
    "                 .str.replace(' ', '_')\n",
    "                 .str.replace('(', '')\n",
    "                 .str.replace(')', '')\n",
    "                 .str.replace('[^a-zA-Z0-9_]', ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccd76099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, threshold=0.7):\n",
    "    \"\"\"Handle missing values based on data type and missingness threshold.\"\"\"\n",
    "    # Drop columns with too many missing values\n",
    "    missing_cols = df.columns[df.isnull().mean() > threshold]\n",
    "    if not missing_cols.empty:\n",
    "        print(f\"Dropping columns with >{threshold*100}% missing values: {list(missing_cols)}\")\n",
    "        df = df.drop(columns=missing_cols)\n",
    "    \n",
    "    # For numeric columns, fill with median\n",
    "    num_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in num_cols:\n",
    "        if df[col].isnull().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            print(f\"Filled missing values in {col} with median: {median_val:.4f}\")\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_cols:\n",
    "        if df[col].isnull().any():\n",
    "            mode_val = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "            print(f\"Filled missing values in {col} with mode: {mode_val}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64dae076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units(df, column, from_unit, to_unit):\n",
    "    \"\"\"Convert units for a column using astropy.\"\"\"\n",
    "    if column in df.columns:\n",
    "        try:\n",
    "            quantity = df[column].values * u.Unit(from_unit)\n",
    "            converted = quantity.to(to_unit).value\n",
    "            df[column] = converted\n",
    "            print(f\"Converted {column} from {from_unit} to {to_unit}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {column}: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00763938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_units(df):\n",
    "    \"\"\"Standardize units across the dataset.\"\"\"\n",
    "    # Example unit conversions - customize based on your data\n",
    "    unit_conversions = {\n",
    "        # 'column_name': ('from_unit', 'to_unit')\n",
    "        'distance': ('Mpc', 'Mpc'),\n",
    "        'redshift': ('', ''),  # Dimensionless\n",
    "        'mass': ('solMass', 'solMass'),\n",
    "        'radius': ('solRad', 'solRad'),\n",
    "        'temperature': ('K', 'K'),\n",
    "    }\n",
    "    \n",
    "    for col, (from_unit, to_unit) in unit_conversions.items():\n",
    "        if col in df.columns:\n",
    "            df = convert_units(df, col, from_unit, to_unit)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01b9630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, subset=None):\n",
    "    \"\"\"Remove duplicate rows, optionally based on a subset of columns.\"\"\"\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates(subset=subset)\n",
    "    removed = initial_rows - len(df)\n",
    "    if removed > 0:\n",
    "        print(f\"Removed {removed} duplicate rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5076d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data_dict):\n",
    "    \"\"\"Clean all datasets in the dictionary.\"\"\"\n",
    "    cleaned_data = {}\n",
    "    \n",
    "    for name, df in tqdm(data_dict.items(), desc=\"Cleaning datasets\"):\n",
    "        print(f\"\\\\nProcessing {name}...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # Apply cleaning steps\n",
    "        df_clean = clean_column_names(df_clean)\n",
    "        df_clean = handle_missing_values(df_clean)\n",
    "        df_clean = standardize_units(df_clean)\n",
    "        \n",
    "        # Remove duplicates (use specific columns if known)\n",
    "        subset = None\n",
    "        if 'id' in df_clean.columns:\n",
    "            subset = ['id']\n",
    "        elif 'name' in df_clean.columns:\n",
    "            subset = ['name']\n",
    "        df_clean = remove_duplicates(df_clean, subset=subset)\n",
    "        \n",
    "        # Store cleaned data\n",
    "        cleaned_data[name] = df_clean\n",
    "        \n",
    "        # Save cleaned data\n",
    "        output_path = PROCESSED_DATA_DIR / f\"{name}_cleaned.csv\"\n",
    "        df_clean.to_csv(output_path, index=False)\n",
    "        print(f\"✓ Saved cleaned data to {output_path}\")\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37496fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning pipeline...\n",
      "Loading raw data files...\n",
      "✓ Loaded act_cmb.csv with shape (4, 5)\n",
      "✓ Loaded combined_cosmology.csv with shape (109309, 29)\n",
      "✓ Loaded cosmological_parameters.csv with shape (8, 6)\n",
      "✓ Loaded des_cosmology.csv with shape (3, 5)\n",
      "✓ Loaded galaxy_distance_evolution.csv with shape (71900, 5)\n",
      "✓ Loaded monte_carlo_times.csv with shape (10000, 7)\n",
      "✓ Loaded nasa_exoplanets.csv with shape (5989, 13)\n",
      "✓ Loaded planck_cosmology.csv with shape (6, 5)\n",
      "✓ Loaded planck_cosmology_2018.csv with shape (6, 3)\n",
      "✓ Loaded scale_factor_evolution.csv with shape (1604, 4)\n",
      "✓ Loaded sh0es_hubble.csv with shape (1, 5)\n",
      "✓ Loaded vacuum_decay_bubbles.csv with shape (25285, 7)\n",
      "✓ Loaded 2dfgrs_galaxies.fits with shape (10000, 11)\n",
      "Error loading sdss_dr16q_quasars.fits: Cannot convert a table with multidimensional columns to a pandas DataFrame. Offending columns are: ['Z_DLA', 'NHI_DLA', 'CONF_DLA', 'PLATE_DUPLICATE', 'MJD_DUPLICATE', 'FIBERID_DUPLICATE', 'SPECTRO_DUPLICATE', 'PSFFLUX', 'PSFFLUX_IVAR', 'PSFMAG', 'PSFMAGERR', 'EXTINCTION']\n",
      "One can filter out such columns using:\n",
      "names = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\n",
      "tbl[names].to_pandas(...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b0e85456bb470289ce43352f3139cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning datasets:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nProcessing act_cmb...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\act_cmb_cleaned.csv\n",
      "\\nProcessing combined_cosmology...\n",
      "Dropping columns with >70.0% missing values: ['parameter', 'value', 'error', 'unit', 'source', 'hubble_constant_h0', 'omega_m', 'omega_lambda', 'w', 'omega_k', 'h0', 'time_to_end_gyr', 'parameter', 'value', 'error', 'scale_factor_a', 'hubble_parameter_h', 'bubble_id', 'bubble_center_x', 'bubble_center_y', 'bubble_radius', 'universe_fraction_remaining']\n",
      "Filled missing values in galaxy_id with median: 511.0000\n",
      "Filled missing values in initial_distance_mpc with median: 2613.0734\n",
      "Filled missing values in time_gyr with median: 42.5000\n",
      "Filled missing values in distance_mpc with median: 23757.2637\n",
      "Filled missing values in simulation_id with median: 71.0000\n",
      "Filled missing values in scenario with mode: Big Freeze\n",
      "Removed 17574 duplicate rows\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\combined_cosmology_cleaned.csv\n",
      "\\nProcessing cosmological_parameters...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\cosmological_parameters_cleaned.csv\n",
      "\\nProcessing des_cosmology...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\des_cosmology_cleaned.csv\n",
      "\\nProcessing galaxy_distance_evolution...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\galaxy_distance_evolution_cleaned.csv\n",
      "\\nProcessing monte_carlo_times...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\monte_carlo_times_cleaned.csv\n",
      "\\nProcessing nasa_exoplanets...\n",
      "Filled missing values in pl_orbper with median: 11.0664\n",
      "Filled missing values in pl_rade with median: 2.4288\n",
      "Filled missing values in pl_bmasse with median: 181.1631\n",
      "Filled missing values in pl_orbsmax with median: 0.1130\n",
      "Filled missing values in st_teff with median: 5551.0000\n",
      "Filled missing values in st_rad with median: 0.9500\n",
      "Filled missing values in st_mass with median: 0.9480\n",
      "Filled missing values in st_met with median: 0.0200\n",
      "Filled missing values in st_age with median: 4.1700\n",
      "Filled missing values in sy_dist with median: 390.3540\n",
      "Filled missing values in st_metratio with mode: [Fe/H]\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\nasa_exoplanets_cleaned.csv\n",
      "\\nProcessing planck_cosmology...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\planck_cosmology_cleaned.csv\n",
      "\\nProcessing planck_cosmology_2018...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\planck_cosmology_2018_cleaned.csv\n",
      "\\nProcessing scale_factor_evolution...\n",
      "Filled missing values in hubble_parameter_h with median: 0.5070\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\scale_factor_evolution_cleaned.csv\n",
      "\\nProcessing sh0es_hubble...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\sh0es_hubble_cleaned.csv\n",
      "\\nProcessing vacuum_decay_bubbles...\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\vacuum_decay_bubbles_cleaned.csv\n",
      "\\nProcessing 2dfgrs_galaxies...\n",
      "Filled missing values in z with median: 0.1154\n",
      "✓ Saved cleaned data to d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\\2dfgrs_galaxies_cleaned.csv\n",
      "\\n=== Data Cleaning Summary ===\n",
      "\\nact_cmb:\n",
      "  Rows: 4\n",
      "  Columns: 5\n",
      "  Missing values: 0\n",
      "\\ncombined_cosmology:\n",
      "  Rows: 91,735\n",
      "  Columns: 7\n",
      "  Missing values: 0\n",
      "\\ncosmological_parameters:\n",
      "  Rows: 8\n",
      "  Columns: 6\n",
      "  Missing values: 0\n",
      "\\ndes_cosmology:\n",
      "  Rows: 3\n",
      "  Columns: 5\n",
      "  Missing values: 0\n",
      "\\ngalaxy_distance_evolution:\n",
      "  Rows: 71,900\n",
      "  Columns: 5\n",
      "  Missing values: 0\n",
      "\\nmonte_carlo_times:\n",
      "  Rows: 10,000\n",
      "  Columns: 7\n",
      "  Missing values: 0\n",
      "\\nnasa_exoplanets:\n",
      "  Rows: 5,989\n",
      "  Columns: 13\n",
      "  Missing values: 0\n",
      "\\nplanck_cosmology:\n",
      "  Rows: 6\n",
      "  Columns: 5\n",
      "  Missing values: 0\n",
      "\\nplanck_cosmology_2018:\n",
      "  Rows: 6\n",
      "  Columns: 3\n",
      "  Missing values: 0\n",
      "\\nscale_factor_evolution:\n",
      "  Rows: 1,604\n",
      "  Columns: 4\n",
      "  Missing values: 0\n",
      "\\nsh0es_hubble:\n",
      "  Rows: 1\n",
      "  Columns: 5\n",
      "  Missing values: 0\n",
      "\\nvacuum_decay_bubbles:\n",
      "  Rows: 25,285\n",
      "  Columns: 7\n",
      "  Missing values: 0\n",
      "\\n2dfgrs_galaxies:\n",
      "  Rows: 10,000\n",
      "  Columns: 11\n",
      "  Missing values: 0\n",
      "\\nData cleaning complete! Cleaned data saved to: d:\\Data Science\\Cosmic_Fate_Simulator\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the data cleaning pipeline.\"\"\"\n",
    "    print(\"Starting data cleaning pipeline...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    raw_data = load_raw_data()\n",
    "    \n",
    "    if not raw_data:\n",
    "        print(\"No data files found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Clean all datasets\n",
    "    cleaned_data = clean_data(raw_data)\n",
    "    \n",
    "    # Generate summary report\n",
    "    print(\"\\\\n=== Data Cleaning Summary ===\")\n",
    "    for name, df in cleaned_data.items():\n",
    "        print(f\"\\\\n{name}:\")\n",
    "        print(f\"  Rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {len(df.columns):,}\")\n",
    "        print(f\"  Missing values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    print(\"\\\\nData cleaning complete! Cleaned data saved to:\", PROCESSED_DATA_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
